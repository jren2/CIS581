{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Fine-Tuned ResNet Video Stream**\n",
        "\n",
        "This model applies our face detection model to a live video stream. Specifically, it uses MTCNN to detect faces in every frame of the video stream, processes them with our model, and then prints the results onto the screen."
      ],
      "metadata": {
        "id": "2kEXmcJp0pKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install facenet_pytorch\n",
        "!pip install mtcnn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwC7NpUN0nOM",
        "outputId": "444afc8f-2be8-4389-bb42-1bf3bb875dff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: facenet_pytorch in /usr/local/lib/python3.10/dist-packages (2.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from facenet_pytorch) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from facenet_pytorch) (2.31.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from facenet_pytorch) (0.16.0+cu118)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from facenet_pytorch) (9.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->facenet_pytorch) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->facenet_pytorch) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->facenet_pytorch) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->facenet_pytorch) (2023.11.17)\n",
            "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->facenet_pytorch) (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision->facenet_pytorch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision->facenet_pytorch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision->facenet_pytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision->facenet_pytorch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision->facenet_pytorch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision->facenet_pytorch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision->facenet_pytorch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->torchvision->facenet_pytorch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->torchvision->facenet_pytorch) (1.3.0)\n",
            "Requirement already satisfied: mtcnn in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from mtcnn) (2.14.0)\n",
            "Requirement already satisfied: opencv-python>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from mtcnn) (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python>=4.1.0->mtcnn) (1.23.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcc4XwFx_pTl"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Javascript, Image, HTML, JSON\n",
        "from google.colab import output\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "from mtcnn import MTCNN\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from functools import wraps\n",
        "import sys\n",
        "import io\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time\n",
        "import dlib\n",
        "from scipy.spatial import Delaunay\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWUslLmyMqlj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f93b8ced-041b-467c-e13e-f42a6b837a9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1SgqS62FW2M"
      },
      "outputs": [],
      "source": [
        "# Function to convert the JavaScript object into an OpenCV image\n",
        "\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params: JavaScript object of image from webcam\n",
        "  Returns: img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# Function to convert OpenCV bounding box image into a base64 byte string\n",
        "# this is used to overlay the bounding box over the video stream\n",
        "\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params: Numpy array to overlay on video stream.\n",
        "  Returns: Base64 image byte string\n",
        "  \"\"\"\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the Python function to stop the video stream\n",
        "\n",
        "def stop_video_stream():\n",
        "    # Add the logic to stop the video stream here\n",
        "    # For demonstration purposes, we'll just print a message\n",
        "    print(\"Stopping video stream\")\n",
        "\n",
        "output.register_callback('stop_video_stream', stop_video_stream)"
      ],
      "metadata": {
        "id": "LO2Q90Gxyoev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsdSVTGBFe1q"
      },
      "outputs": [],
      "source": [
        "# JavaScript to properly create our live video stream using a webcam as the input\n",
        "\n",
        "def video_stream():\n",
        "    js = Javascript('''\n",
        "        let video;\n",
        "        let videoStream;\n",
        "        let captureCanvas;\n",
        "\n",
        "        let mainDiv = null;\n",
        "        let imgElement;\n",
        "        let labelElement;\n",
        "\n",
        "        let pendingResolve = null;\n",
        "        let shutdown = false;\n",
        "\n",
        "        function stopVideoStream() {\n",
        "            shutdown = true;\n",
        "            removeDom();\n",
        "        }\n",
        "\n",
        "        function removeDom() {\n",
        "            videoStream.getVideoTracks()[0].stop();\n",
        "            video.remove();\n",
        "            video = null;\n",
        "            videoStream = null;\n",
        "            captureCanvas = null;\n",
        "\n",
        "            mainDiv.remove();\n",
        "            mainDiv = null;\n",
        "            imgElement = null;\n",
        "            labelElement = null;\n",
        "        }\n",
        "\n",
        "        function onAnimationFrame() {\n",
        "            if (!shutdown) {\n",
        "                window.requestAnimationFrame(onAnimationFrame);\n",
        "            }\n",
        "            if (pendingResolve) {\n",
        "                var result = \"\";\n",
        "                if (!shutdown) {\n",
        "                    captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "                    result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "                }\n",
        "                var lp = pendingResolve;\n",
        "                pendingResolve = null;\n",
        "                lp(result);\n",
        "            }\n",
        "        }\n",
        "\n",
        "        async function createDom() {\n",
        "            if (mainDiv !== null) {\n",
        "                return videoStream;\n",
        "            }\n",
        "\n",
        "            mainDiv = document.createElement('div');\n",
        "            mainDiv.style.padding = '4px';\n",
        "            mainDiv.style.border = '2px solid white';\n",
        "            mainDiv.style.maxWidth = '800px';\n",
        "            mainDiv.style.width = '792px';\n",
        "            document.body.appendChild(mainDiv);\n",
        "\n",
        "            const modelOut = document.createElement('div');\n",
        "            modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "            labelElement = document.createElement('span');\n",
        "            labelElement.innerText = 'Data is empty';\n",
        "            labelElement.style.fontWeight = 'bold';\n",
        "            modelOut.appendChild(labelElement);\n",
        "            mainDiv.appendChild(modelOut);\n",
        "\n",
        "            video = document.createElement('video');\n",
        "            video.style.display = 'block';\n",
        "            video.width = window.innerWidth * 0.6;\n",
        "            video.setAttribute('playsinline', '');\n",
        "            video.onclick = () => { shutdown = true; };\n",
        "            videoStream = await navigator.mediaDevices.getUserMedia(\n",
        "                { video: { facingMode: \"environment\" } });\n",
        "            mainDiv.appendChild(video);\n",
        "\n",
        "            imgElement = document.createElement('img');\n",
        "            imgElement.style.position = 'absolute';\n",
        "            imgElement.style.zIndex = 1;\n",
        "            imgElement.onclick = () => { shutdown = true; };\n",
        "            mainDiv.appendChild(imgElement);\n",
        "\n",
        "            const instruction = document.createElement('div');\n",
        "            instruction.innerHTML =\n",
        "                '<span style=\"color: white; font-weight: bold;\">' +\n",
        "                'Click here or on the video to stop live stream</span>';\n",
        "            mainDiv.appendChild(instruction);\n",
        "\n",
        "            // Add stop button\n",
        "            const stopButton = document.createElement('button');\n",
        "            stopButton.innerText = 'Stop Video Stream';\n",
        "            stopButton.onclick = () => { shutdown = true; };\n",
        "            mainDiv.appendChild(stopButton);\n",
        "\n",
        "            instruction.onclick = () => { shutdown = true; };\n",
        "\n",
        "            video.srcObject = videoStream;\n",
        "            await video.play();\n",
        "\n",
        "            captureCanvas = document.createElement('canvas');\n",
        "            captureCanvas.width = 640;\n",
        "            captureCanvas.height = 480;\n",
        "            window.requestAnimationFrame(onAnimationFrame);\n",
        "\n",
        "            return videoStream;\n",
        "        }\n",
        "\n",
        "        async function stream_frame(label, imgData) {\n",
        "            if (shutdown) {\n",
        "                removeDom();\n",
        "                shutdown = false;\n",
        "                return '';\n",
        "            }\n",
        "\n",
        "            var preCreate = Date.now();\n",
        "            videoStream = await createDom();\n",
        "\n",
        "            var preShow = Date.now();\n",
        "            if (label != \"\") {\n",
        "                labelElement.innerHTML = label;\n",
        "            }\n",
        "\n",
        "            if (imgData != \"\") {\n",
        "                var videoRect = video.getClientRects()[0];\n",
        "                imgElement.style.top = videoRect.top + \"px\";\n",
        "                imgElement.style.left = videoRect.left + \"px\";\n",
        "                imgElement.style.width = videoRect.width + \"px\";\n",
        "                imgElement.style.height = videoRect.height + \"px\";\n",
        "                imgElement.src = imgData;\n",
        "            }\n",
        "\n",
        "            var preCapture = Date.now();\n",
        "            var result = await new Promise(function (resolve, reject) {\n",
        "                pendingResolve = resolve;\n",
        "            });\n",
        "            shutdown = false;\n",
        "\n",
        "            return {\n",
        "                'create': preShow - preCreate,\n",
        "                'show': preCapture - preShow,\n",
        "                'capture': Date.now() - preCapture,\n",
        "                'img': result\n",
        "            };\n",
        "        }\n",
        "    ''')\n",
        "\n",
        "    display(js)\n",
        "\n",
        "def video_frame(label, bbox):\n",
        "    data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "    if 'error' in data:\n",
        "        # Handle the error, e.g., video stream has been stopped\n",
        "        print(data['error'])\n",
        "        return None\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><strong> Video Stream Capture </strong></h1>"
      ],
      "metadata": {
        "id": "xbuQsGWbYztD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4><strong> Face detection model </strong></h4>\n",
        "There are two options for face detection from video stream: <br />\n",
        "1) Haar cascade classifier <br />\n",
        "2) MTCNN <br />\n",
        "<br />\n",
        "Haar cascade classifier is significantly quicker and is able to track your face more frequently, however it is fairly inaccurate. For example, it does not detect faces which are slightly tilted. MTCNN is far slower (roughly 1.5 frames per second) than Haar but is more accurate - it allows for tilted faces. <br />\n",
        "<br />\n",
        "To use the Haar cascade classifier, set useHaar to true. To use MTCNN, set useHaar to false."
      ],
      "metadata": {
        "id": "ujL5X5BlY3W3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "useHaar = True"
      ],
      "metadata": {
        "id": "5wa5mMvhZa0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<strong> Loading the fine-tuned model </strong>"
      ],
      "metadata": {
        "id": "TulYqdvmhRv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "EDIT HERE: Change model_path to point to your face_recog_2.h5 path\n",
        "\"\"\"\n",
        "model_path = '/content/drive/My Drive/581 Final Project/Code Workspace/face_recog_2.h5'\n",
        "model = torch.load(model_path)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "tgRAHoADhQB9",
        "outputId": "6dc8078d-86ca-448d-d588-fed983daab1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-22a52dd47468>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/My Drive/581 Final Project/Code Workspace/face_recog_2.h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<strong> MTCNN Model </strong>"
      ],
      "metadata": {
        "id": "3Mg5S38CbefI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "MTCNN produces output which moves our video stream\n",
        "To prevent this, we use a wrapper function to capture output\n",
        "\"\"\"\n",
        "def capture_output(func):\n",
        "    @wraps(func)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        old_stdout = sys.stdout\n",
        "        new_stdout = io.StringIO()\n",
        "        sys.stdout = new_stdout\n",
        "        try:\n",
        "            return func(*args, **kwargs)\n",
        "        finally:\n",
        "            sys.stdout = old_stdout\n",
        "\n",
        "    return wrapper"
      ],
      "metadata": {
        "id": "ZXGmTZHUXPch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Return the faces that are confidently faces\n",
        "def faceDetection(frame):\n",
        "    detector = MTCNN()\n",
        "    intermediate_func = capture_output(detector.detect_faces)\n",
        "    boxes = intermediate_func(frame)\n",
        "    if boxes:\n",
        "        confident_boxes = [single_box['box'] for single_box in boxes if single_box['confidence'] > 0.9]\n",
        "        return confident_boxes\n",
        "    else:\n",
        "        return []"
      ],
      "metadata": {
        "id": "RXJ8RCnTXTWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<strong> Haar Cascade Classifer Model </strong>"
      ],
      "metadata": {
        "id": "zyoIk-Q-b5JK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Haar Cascade face detection model\n",
        "face_cascade = cv2.CascadeClassifier(cv2.samples.findFile(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'))"
      ],
      "metadata": {
        "id": "5WSG83saRvIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<strong> Video Stream </strong>"
      ],
      "metadata": {
        "id": "3c0JZRZ_ekiJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3onKWfSVFi3_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e5866915-a424-4844-d838-14bb52195147"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        let video;\n",
              "        let videoStream;\n",
              "        let captureCanvas;\n",
              "\n",
              "        let mainDiv = null;\n",
              "        let imgElement;\n",
              "        let labelElement;\n",
              "\n",
              "        let pendingResolve = null;\n",
              "        let shutdown = false;\n",
              "\n",
              "        function stopVideoStream() {\n",
              "            shutdown = true;\n",
              "            removeDom();\n",
              "        }\n",
              "\n",
              "        function removeDom() {\n",
              "            videoStream.getVideoTracks()[0].stop();\n",
              "            video.remove();\n",
              "            video = null;\n",
              "            videoStream = null;\n",
              "            captureCanvas = null;\n",
              "\n",
              "            mainDiv.remove();\n",
              "            mainDiv = null;\n",
              "            imgElement = null;\n",
              "            labelElement = null;\n",
              "        }\n",
              "\n",
              "        function onAnimationFrame() {\n",
              "            if (!shutdown) {\n",
              "                window.requestAnimationFrame(onAnimationFrame);\n",
              "            }\n",
              "            if (pendingResolve) {\n",
              "                var result = \"\";\n",
              "                if (!shutdown) {\n",
              "                    captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
              "                    result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
              "                }\n",
              "                var lp = pendingResolve;\n",
              "                pendingResolve = null;\n",
              "                lp(result);\n",
              "            }\n",
              "        }\n",
              "\n",
              "        async function createDom() {\n",
              "            if (mainDiv !== null) {\n",
              "                return videoStream;\n",
              "            }\n",
              "\n",
              "            mainDiv = document.createElement('div');\n",
              "            mainDiv.style.padding = '4px';\n",
              "            mainDiv.style.border = '2px solid white';\n",
              "            mainDiv.style.maxWidth = '800px';\n",
              "            mainDiv.style.width = '792px';\n",
              "            document.body.appendChild(mainDiv);\n",
              "\n",
              "            const modelOut = document.createElement('div');\n",
              "            modelOut.innerHTML = \"<span>Status:</span>\";\n",
              "            labelElement = document.createElement('span');\n",
              "            labelElement.innerText = 'Data is empty';\n",
              "            labelElement.style.fontWeight = 'bold';\n",
              "            modelOut.appendChild(labelElement);\n",
              "            mainDiv.appendChild(modelOut);\n",
              "\n",
              "            video = document.createElement('video');\n",
              "            video.style.display = 'block';\n",
              "            video.width = window.innerWidth * 0.6;\n",
              "            video.setAttribute('playsinline', '');\n",
              "            video.onclick = () => { shutdown = true; };\n",
              "            videoStream = await navigator.mediaDevices.getUserMedia(\n",
              "                { video: { facingMode: \"environment\" } });\n",
              "            mainDiv.appendChild(video);\n",
              "\n",
              "            imgElement = document.createElement('img');\n",
              "            imgElement.style.position = 'absolute';\n",
              "            imgElement.style.zIndex = 1;\n",
              "            imgElement.onclick = () => { shutdown = true; };\n",
              "            mainDiv.appendChild(imgElement);\n",
              "\n",
              "            const instruction = document.createElement('div');\n",
              "            instruction.innerHTML =\n",
              "                '<span style=\"color: white; font-weight: bold;\">' +\n",
              "                'Click here or on the video to stop live stream</span>';\n",
              "            mainDiv.appendChild(instruction);\n",
              "\n",
              "            // Add stop button\n",
              "            const stopButton = document.createElement('button');\n",
              "            stopButton.innerText = 'Stop Video Stream';\n",
              "            stopButton.onclick = () => { shutdown = true; };\n",
              "            mainDiv.appendChild(stopButton);\n",
              "\n",
              "            instruction.onclick = () => { shutdown = true; };\n",
              "\n",
              "            video.srcObject = videoStream;\n",
              "            await video.play();\n",
              "\n",
              "            captureCanvas = document.createElement('canvas');\n",
              "            captureCanvas.width = 640;\n",
              "            captureCanvas.height = 480;\n",
              "            window.requestAnimationFrame(onAnimationFrame);\n",
              "\n",
              "            return videoStream;\n",
              "        }\n",
              "\n",
              "        async function stream_frame(label, imgData) {\n",
              "            if (shutdown) {\n",
              "                removeDom();\n",
              "                shutdown = false;\n",
              "                return '';\n",
              "            }\n",
              "\n",
              "            var preCreate = Date.now();\n",
              "            videoStream = await createDom();\n",
              "\n",
              "            var preShow = Date.now();\n",
              "            if (label != \"\") {\n",
              "                labelElement.innerHTML = label;\n",
              "            }\n",
              "\n",
              "            if (imgData != \"\") {\n",
              "                var videoRect = video.getClientRects()[0];\n",
              "                imgElement.style.top = videoRect.top + \"px\";\n",
              "                imgElement.style.left = videoRect.left + \"px\";\n",
              "                imgElement.style.width = videoRect.width + \"px\";\n",
              "                imgElement.style.height = videoRect.height + \"px\";\n",
              "                imgElement.src = imgData;\n",
              "            }\n",
              "\n",
              "            var preCapture = Date.now();\n",
              "            var result = await new Promise(function (resolve, reject) {\n",
              "                pendingResolve = resolve;\n",
              "            });\n",
              "            shutdown = false;\n",
              "\n",
              "            return {\n",
              "                'create': preShow - preCreate,\n",
              "                'show': preCapture - preShow,\n",
              "                'capture': Date.now() - preCapture,\n",
              "                'img': result\n",
              "            };\n",
              "        }\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-a4d79f467405>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m       \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjs_to_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjs_reply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"img\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_RGB2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m       \u001b[0mfaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_cascade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaleFactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminNeighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminSize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m       \u001b[0;31m# If using MTCNN, detect with faceDetection MTCNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "label_html = 'Capturing...'\n",
        "video_stream()\n",
        "\n",
        "# Initialize bounding box to empty\n",
        "bbox = ''\n",
        "\n",
        "captured_img = None\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((160, 160)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "while True:\n",
        "    predictions = []\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "    if js_reply == None:\n",
        "        print(\"js_reply is none\")\n",
        "        break\n",
        "\n",
        "    faces = []\n",
        "    bbox_array = np.zeros([480, 640, 4], dtype=np.uint8)\n",
        "\n",
        "    if useHaar:\n",
        "      # If using Haar, detect with face_cascade\n",
        "      img = js_to_image(js_reply[\"img\"])\n",
        "      gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "      faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(50, 50))\n",
        "    else:\n",
        "      # If using MTCNN, detect with faceDetection MTCNN\n",
        "      image_bytes = b64decode(js_reply['img'].split(',')[1])\n",
        "      img = Image.open(BytesIO(image_bytes))\n",
        "      frame = np.array(img.convert('RGB'), 'f')\n",
        "      faces = faceDetection(frame)\n",
        "\n",
        "    # If we've detected a face, display it\n",
        "    if len(faces) > 0:\n",
        "      for (x, y, w, h) in faces:\n",
        "          if useHaar:\n",
        "            # If using Haar, crop from the numpy image\n",
        "            cropped_face = Image.fromarray(img[y:y + h, x:x + w])\n",
        "          else:\n",
        "            # If using MTCNN, crop from PIL\n",
        "            cropped_face = img.crop([x, y, x + w, y + h])\n",
        "\n",
        "          # Transform before predicting\n",
        "          input_tensor = transform(cropped_face)\n",
        "          input_tensor = input_tensor.unsqueeze(0)\n",
        "          input_tensor = input_tensor.to(\"cpu\")\n",
        "          with torch.no_grad():\n",
        "            output = model(input_tensor)\n",
        "            leo, not_leo = torch.sigmoid(output[0])\n",
        "\n",
        "          # If the face is more likely to be leo, display leo\n",
        "          if leo > not_leo:\n",
        "            display_string = f'Leonardo DiCaprio ({leo * 100:.2f}%)'\n",
        "          else:\n",
        "            display_string = f'NOT Leonardo DiCaprio ({not_leo * 100:.2f}%)'\n",
        "\n",
        "          bbox_array = cv2.rectangle(bbox_array, (x, y), (x + w, y + h), (255, 255, 255), 2)\n",
        "          cv2.putText(bbox_array, display_string, (x, y - 10), cv2.FONT_HERSHEY_TRIPLEX, 0.6, (255, 255, 255), 1)\n",
        "\n",
        "\n",
        "    bbox_array[:, :, 3] = (bbox_array.max(axis=2) > 0).astype(int) * 255\n",
        "\n",
        "    bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "    bbox = bbox_bytes\n",
        "\n",
        "    captured_img = js_to_image(js_reply[\"img\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-T3-ZYBngZlE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}